# ============================================================
# personal-mcp — config.yaml
# ============================================================

# ── Server ──────────────────────────────────────────────────
server:
  host: 0.0.0.0
  port: 8000
  admin_token: "change-me-please"   # or set ADMIN_TOKEN env var

# ── Storage ─────────────────────────────────────────────────
db_path: db/profile.db
cache_dir: .cache
cache_ttl_hours: 6
watch_interval_minutes: 120

# Auto-export entities to YAML after ingestion (for manual editing workflow)
# If true, creates data/<source>_export.yaml files with entity_id
# You can then edit these files and update DB with: python ingest.py --yaml-update --file data/<source>_export.yaml
auto_export_yaml: true

# ── Your identity ────────────────────────────────────────────
# fixed values, will be added to the generic endpoint
# supports multi languages, e.g. identity.de.name → "name": "Nicky Reinert" in the German version of the profile
# dynamic attributes, every sub attribute will appear in the JSON output of the generic endpoint, e.g. identity.name → "name": "Nicky Reinert"
identity:
  de:
    name: Nicky Reinert
    tagline: "Understand Data → Connect Interfaces → Automate Processes → Solve Problems"
    location: Berlin, Germany
    github_url: https://github.com/nickyreinert
    medium_url: https://nickyreinert.medium.com
    blog_url: https://nickyreinert.de
    linkedin_url: https://www.linkedin.com/in/nickyreinert
    description: "Persönliches Profil von Nicky Reinert, das berufliche Erfahrungen, Projekte und Veröffentlichungen zeigt."
  en:
    name: Nicky Reinert
    tagline: "Understand Data → Connect Interfaces → Automate Processes → Solve Problems"
    location: Berlin, Germany
    github_url: https://github.com/nickyreinert
    medium_url: https://nickyreinert.medium.com
    blog_url: https://nickyreinert.de
    linkedin_url: https://www.linkedin.com/in/nickyreinert
    description: "Personal profile of Nicky Reinert, showcasing professional experience, projects, and publications."

# ── LLM Enrichment ──────────────────────────────────────────
# backend: groq | ollama | none
#
# GROQ (recommended: fast, free tier, ~100 calls/day free):
#   pip install groq && export GROQ_API_KEY=gsk_...
#   models: llama3-8b-8192 (fast) | llama3-70b-8192 (smart) | mixtral-8x7b-32768
#
# Ollama (fully local, zero API cost, privacy-first):
#   brew install ollama && ollama serve && ollama pull llama3
#   models: llama3 | mistral | phi3 | gemma2 | qwen2
#
llm:
  backend: ollama
  model: mistral-small:24b-instruct-2501-q4_K_M
  groq_api_key: ""          # or set GROQ_API_KEY env var
  ollama_url: http://localhost:11434

# ── Statoc export ──────────────────────────────────────────
# Fallback yaml in case the MCP server is not response, according to groundinpage rules https://groundingpage.com
# TOOD: whats the best name here?
static: llm.yaml


# ── Stages (WHERE you worked/studied) ──────────────────────────
# Stages are companies, institutions, professional roles, education, certifications
#
# Workflow:
#   1. First run: Parses PDF using LLM → creates linkedin_profile.pdf.yaml cache
#   2. Manual editing: Edit linkedin_profile.pdf.yaml to refine content
#   3. Subsequent runs: Loads from .yaml cache (fast, no LLM needed)
#   4. Disable: Set enabled=false to skip stages processing entirely
#
# source_path: Path to LinkedIn profile PDF export
#   - If <source_path>.yaml exists → loads from YAML (user-edited cache)
#   - If <source_path>.yaml missing → parses PDF with LLM and saves YAML
#
stages:
  enabled: true                      # Set false to skip stages processing
  source_type: linkedin_pdf          # Only linkedin_pdf supported (auto-caching to YAML)
  source_path: linkedin_profile.pdf  # LinkedIn export PDF file

# ── Sources for oeuvre ─────────────────────────────────────────────────
# Configure the sources to fetch your data from. Each source can be enabled/disabled independently.
# source names must be unique, e.g. github, medium, blog_1, blog_2, etc. but you can define as many as you like.
oeuvre_sources:

  github:
    enabled: true
    connector: github_api
    url: https://api.github.com/users/nickyreinert/repos
    sub_type_override: coding   # Override default sub_type (coding/blog_post/article/book/website/podcast/video)
    limit: 0                    # 0 = all, otherwise integer limit on number of repos to fetch
    llm-processing: true
    fetch_readmes: true         # true = richer descriptions, slower

  medium_init:
     # for initial fetch of your articles, go to the list fo alll your stories and copy the whole dome into an html file
    enabled: false
    connector: medium_raw
    url: file://data/Medium.html  # Public RSS feed
    

  medium:
    enabled: true
    connector: manual
    url: https://nickyreinert.medium.com/  # Profile page - shows all articles
    sub_type_override: article
    limit: 0  # 0 = all articles found
    cache_ttl_hours: 168  # 7 days
    llm-processing: true

    connector-setup:
      # Select all article links on profile page
      # Medium uses various link formats, try broad selector then filter
      post_url_selector: 'a[href*="medium.com/@nickyreinert"]'
      # When visiting each article:
      post-title-selector: 'h1'
      post-content-selector: 'article'
      post-published-date-selector: 'meta[property="article:published_time"]'
      post-description-selector: 'meta[property="og:description"]'   

  blog_1:
    enabled: true
    connector: rss
    url: https://nickyreinert.de/index.xml
    sub_type_override: blog_post  # Override default sub_type
    llm-processing: true
    limit: 0                    # 0 = all, otherwise integer limit on number of posts to fetch

  # Example: Sitemap connector
  # Mode 1: Multi-entity (each page = separate entity)
  # blog_sitemap_pages:
  #   enabled: false
  #   connector: sitemap
  #   url: https://example.com/sitemap.xml
  #   sub_type_override: blog_post
  #   limit: 20                  # Process only first 20 URLs from sitemap
  #   cache_ttl_hours: 336       # 14 days cache TTL
  #   llm-processing: true
  #   single-entity: false       # Each page becomes a separate entity (default)
  #   cache-file: file://data/blog_sitemap.yaml  # Optional: cache for manual editing
  #
  #   connector-setup:
  #     post-title-selector: h1
  #     post-content-selector: article
  #     post-published-date-selector: 'time[datetime]'
  #     post-description-selector: 'meta[name="description"]'
  
  # Mode 2: Single-entity (whole site = one entity)
  # project_website:
  #   enabled: false
  #   connector: sitemap
  #   url: https://example.com/sitemap.xml
  #   sub_type_override: website
  #   llm-processing: true
  #   single-entity: true        # Treat entire site as one entity
  #   cache-file: file://data/project_sitemap.yaml  # Optional: cache for manual editing
  #
  #   connector-setup:
  #     post-title-selector: h1
  #     post-content-selector: main
  #     post-description-selector: 'meta[name="description"]'

# ── Internationalization (i18n) ──────────────────────────────────────────────
# target_languages: languages to translate into (besides the default 'en').
# Run: python -m llm.translator   or   POST /admin/translate
#
# batch_sleep_seconds: pause between LLM calls (GROQ free tier: ~30 req/min)
# A full German translation of ~200 entities takes roughly 3-5 minutes.
#
i18n:
  # while you can add as many languages to the identity area on top, if you want the dynamic part to be 
  # translated automatically, you must add the language code here, e.g. "de" for German, "en" for English, "fr" for French, etc.
  # the LLM will take over the translation part then
  target_languages:
    - de                          # German — primary second language
    - en                          # English — primary second language
  batch_sleep_seconds: 0.4        # ~25 req/min → safe under GROQ free tier